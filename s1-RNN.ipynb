{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import spacy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# use torchtext to load train, test, pretrainede datasets\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.data import TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build csv from text for training by torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "5000 positive tweets loaded\n",
      "5000 negative tweets loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train list, label list\n",
    "print(\"Loading dataset...\")\n",
    "fp = open(\"datasets/train_pos_tidy.txt\", \"r\")\n",
    "train_pos = fp.readlines()\n",
    "train_pos = train_pos[:5000]\n",
    "label_pos = ['1' for x in range(len(train_pos))] # positive label:1\n",
    "print(len(train_pos), \"positive tweets loaded\")\n",
    "fp.close()\n",
    "fn = open(\"datasets/train_neg_tidy.txt\", \"r\")\n",
    "train_neg = fn.readlines()\n",
    "train_neg = train_neg[:5000]\n",
    "label_neg = ['0' for x in range(len(train_neg))] # negative label:0\n",
    "print(len(train_neg), \"negative tweets loaded\\n\")\n",
    "fn.close()\n",
    "train_data = train_pos + train_neg\n",
    "train_label = label_pos + label_neg   \n",
    "\n",
    "# key in dictionary is the column name in csv\n",
    "dataframe = pd.DataFrame({'text':train_data,'label':train_label})\n",
    "# save DataFrame as csv\n",
    "dataframe.to_csv(\"datasets/train.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build training input by torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en')\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False,dtype=torch.float)\n",
    "\n",
    "trn_datafields = [(\"text\", TEXT),(\"label\", LABEL)]\n",
    "trn = TabularDataset(\n",
    "               path=\"datasets/train.csv\", # training data csv path\n",
    "               format='csv',\n",
    "               skip_header=True, # skip csv header\n",
    "               fields=trn_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'label'])\n",
      "{'text': ['dunno', 'justin', 'read', 'mention', 'not', 'only', 'justin', 'and', 'god', 'know', 'but', 'hope', 'you', 'follow', '#', 'believe'], 'label': '1'}\n",
      "['dunno', 'justin', 'read', 'mention', 'not', 'only', 'justin', 'and', 'god', 'know', 'but', 'hope', 'you', 'follow', '#', 'believe']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(trn[0].__dict__.keys())\n",
    "print(vars(trn.examples[0]))\n",
    "print(trn[0].text[:])\n",
    "print(trn[5000].label[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 7000\n",
      "Number of validation examples: 3000\n"
     ]
    }
   ],
   "source": [
    "# split data: training, validation \n",
    "train_data, valid_data = trn.split(random_state=random.seed(SEED),split_ratio=0.7) # 70%:30%\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.twitter.27B.100d\")\n",
    "# load pre-trained emmbedding text\n",
    "# here we use the text name limited in build_vocab, like 'charngram.100d', 'fasttext.en.300d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Field' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-c9cbd2bc6780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(LABEL.vocab.stoi)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Field' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "#print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "#print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=False,\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10198, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0738,  0.2296,  0.1619,  ..., -0.5719,  0.5669, -0.0974],\n",
       "        ...,\n",
       "        [ 0.4244,  0.1037, -1.3166,  ...,  0.4221,  0.4951,  0.2900],\n",
       "        [-0.2551, -0.4357,  0.0260,  ..., -0.4315, -0.0373, -0.2629],\n",
       "        [ 0.5168, -0.2330,  0.3913,  ...,  0.5349, -0.5232, -0.0990]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instructor():\n",
    "# modle process: train, evaluate, save, load, predict\n",
    "    def __init__(self,model, optimizer, criterion):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "    def train(self,iterator):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "            loss = self.criterion(predictions, batch.label)\n",
    "\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def evaluate(self,iterator):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch in iterator:\n",
    "\n",
    "                predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "                loss = self.criterion(predictions, batch.label)\n",
    "\n",
    "                acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    def save(self,state,dir):\n",
    "        if not os.path.isdir('model_trained'): #find the file for model saving and loading\n",
    "            os.mkdir('model_trained')\n",
    "        torch.save(state, dir)\n",
    "        print('--- Save last model state')\n",
    "\n",
    "    def load(self,dir):\n",
    "        if not os.path.isdir('model_trained'): #find the file for model saving and loading\n",
    "            os.mkdir('model_trained')\n",
    "        checkpoint = torch.load(dir)\n",
    "        self.model.load_state_dict(checkpoint['net'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print('--- Load last model state')\n",
    "        print('start epoch:',start_epoch)\n",
    "        return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "instructor =  Instructor(model, optimizer, criterion)#class Instructor for model processing\n",
    "\n",
    "epoch_start = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.562 | Train Acc: 69.23% | Val. Loss: 0.497 | Val. Acc: 74.44% |\n",
      "| Epoch: 02 | Train Loss: 0.478 | Train Acc: 75.97% | Val. Loss: 0.470 | Val. Acc: 76.11% |\n",
      "| Epoch: 03 | Train Loss: 0.444 | Train Acc: 78.08% | Val. Loss: 0.477 | Val. Acc: 76.32% |\n",
      "| Epoch: 04 | Train Loss: 0.417 | Train Acc: 79.57% | Val. Loss: 0.444 | Val. Acc: 78.63% |\n",
      "| Epoch: 05 | Train Loss: 0.386 | Train Acc: 81.56% | Val. Loss: 0.440 | Val. Acc: 79.37% |\n",
      "| Epoch: 06 | Train Loss: 0.354 | Train Acc: 83.68% | Val. Loss: 0.445 | Val. Acc: 79.07% |\n",
      "| Epoch: 07 | Train Loss: 0.326 | Train Acc: 85.45% | Val. Loss: 0.436 | Val. Acc: 79.00% |\n",
      "| Epoch: 08 | Train Loss: 0.299 | Train Acc: 86.70% | Val. Loss: 0.438 | Val. Acc: 79.89% |\n",
      "| Epoch: 09 | Train Loss: 0.286 | Train Acc: 87.52% | Val. Loss: 0.470 | Val. Acc: 79.73% |\n",
      "| Epoch: 10 | Train Loss: 0.264 | Train Acc: 88.72% | Val. Loss: 0.528 | Val. Acc: 77.98% |\n",
      "| Epoch: 11 | Train Loss: 0.225 | Train Acc: 90.55% | Val. Loss: 0.483 | Val. Acc: 79.11% |\n",
      "| Epoch: 12 | Train Loss: 0.220 | Train Acc: 90.64% | Val. Loss: 0.534 | Val. Acc: 78.78% |\n",
      "| Epoch: 13 | Train Loss: 0.203 | Train Acc: 91.34% | Val. Loss: 0.497 | Val. Acc: 79.02% |\n",
      "| Epoch: 14 | Train Loss: 0.196 | Train Acc: 91.96% | Val. Loss: 0.524 | Val. Acc: 78.58% |\n",
      "| Epoch: 15 | Train Loss: 0.176 | Train Acc: 92.57% | Val. Loss: 0.543 | Val. Acc: 78.54% |\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "for epoch in range(epoch_start,epoch_start+N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = instructor.train(train_iterator)\n",
    "    valid_loss, valid_acc = instructor.evaluate(valid_iterator)\n",
    "    \n",
    "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "epoch_start = epoch_start+N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Save last model state\n"
     ]
    }
   ],
   "source": [
    "# Save last model state\n",
    "state = {'net':model.state_dict(), 'optimizer':optimizer.state_dict(), 'epoch':epoch}\n",
    "dir = './model_trained/LSTM_version_2.' #model path\n",
    "instructor.save(state,dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Load last model state\n",
      "start epoch: 6\n"
     ]
    }
   ],
   "source": [
    "# Load last model state\n",
    "dir = './model_trained/LSTM_version_1.' #model path\n",
    "epoch_start = instructor.load(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "10000 test tweets loaded\n"
     ]
    }
   ],
   "source": [
    "#test predition\n",
    "print(\"Loading dataset...\")\n",
    "fp = open(\"datasets/test_data.txt\", \"r\")\n",
    "test_data = fp.readlines()\n",
    "print(len(test_data), \"test tweets loaded\")\n",
    "fp.close()\n",
    "\n",
    "def predict_sentiment(sentence):\n",
    "    nlp = spacy.load('en')\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    if prediction.item() <= 0.5:\n",
    "        pred = -1\n",
    "    else:\n",
    "        pred = 1 \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "out_dir = './out/'\n",
    "localtime = time.asctime(time.localtime(time.time()))\n",
    "fp = open(out_dir + \"submission \" + localtime[4:-5] + \".csv\", \"w\")\n",
    "fieldnames = ['Id', 'Prediction']\n",
    "writer = csv.DictWriter(fp, fieldnames=fieldnames)\n",
    "writer.writeheader()\n",
    "\n",
    "print(\"Generating predictions...\\n\")\n",
    "\n",
    "for tweet in test_data:\n",
    "    i, t = tweet.split(\",\", maxsplit=1)  # Splitting the index from the tweet text\n",
    "    prediction = predict_sentiment(t)\n",
    "    writer.writerow({'Id': str(i), 'Prediction': str(prediction)})\n",
    "fp.close()\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
