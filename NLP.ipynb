{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "100000 positive tweets loaded\n",
      "100000 negative tweets loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load original tweets\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "fp = open(\"twitter-datasets/train_pos.txt\", \"r\")\n",
    "train_pos = fp.readlines()\n",
    "print(len(train_pos), \"positive tweets loaded\")\n",
    "fp.close()\n",
    "fn = open(\"twitter-datasets/train_neg.txt\", \"r\")\n",
    "train_neg = fn.readlines()\n",
    "print(len(train_neg), \"negative tweets loaded\\n\")\n",
    "fn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove twitter handles (<user>,<url>)\n",
    "\n",
    "pattern = \"<[\\w]*>\"\n",
    "def remove_pattern(input_text):\n",
    "    r = re.findall(pattern, input_text)\n",
    "    for i in r:\n",
    "        input_text = re.sub(i, '', input_text)\n",
    "    return input_text\n",
    "\n",
    "# remove_pattern(\"this function removes both user <user> and url <url>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization\n",
    "\n",
    "lookup_dict = {'b/c':'because','b4':'before','bfn':'bye for now','bgd':'background','br':'best regards',\n",
    "              'chk':'check','cld':'could','clk':'click','cre8':'create','dm':'direct message',\n",
    "              'em':'email','ema':'email address','f2f':'face to face','fab':'fabulous',\n",
    "               'ff':'follow friday','ftl':'for the loss','ftw':'for the win','ic':'i see',\n",
    "               'idk':'i dont know','kk':'cool cool','mrt':'modified retweet','mtf':'more to follow',\n",
    "               'nts':'note to self','oh':'overheard','prt':'please retweet','rt':'retweet',\n",
    "               'tmb':'tweet me back','u':'you','woz':'was','wtv':'whatever',\n",
    "               'ykyat':'you know youâ€™re addicted to','yoyo':'you are on your own'\n",
    "              }\n",
    "\n",
    "def standardization(input_text):\n",
    "    words = input_text.split() \n",
    "    standardized_words = [] \n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        standardized_words.append(word)\n",
    "    return \" \".join(standardized_words)\n",
    "\n",
    "# standardization(\"this function converts RT back to retweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numbers and punctuations, and convert to lowercase\n",
    "\n",
    "def remove_punctuation(input_text):\n",
    "    words = re.findall(r'[a-zA-Z#]+', input_text)\n",
    "    lower_words = [word.lower() for word in words]\n",
    "    return ' '.join(lower_words)\n",
    "\n",
    "# print(remove_punctuation(\"This! Function@ Removes numBERs8079 1421 aNd punctuationS ? ! $@!\"))\n",
    "# print(remove_punctuation(\"IT ONLY KEEPS LOWERCASE CHARACTER and #HASHTAGS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(input_text):\n",
    "    words = input_text.split()\n",
    "    lemmatized_words = [lem.lemmatize(word,\"v\") for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# lemmatization('this function converts play played plays and playing to play')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove short words\n",
    "\n",
    "def remove_short(input_text):\n",
    "    words = input_text.split()\n",
    "    long_words = [word for word in words if len(word)>2]\n",
    "    return ' '.join(long_words)\n",
    "\n",
    "# remove_short(\"this function removes short words like hm and oh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 stopwords loaded\n"
     ]
    }
   ],
   "source": [
    "# noise removal\n",
    "\n",
    "f = open(\"stopwords.txt\", \"r\")\n",
    "noise_list = f.read().splitlines()\n",
    "print(len(noise_list), \"stopwords loaded\")\n",
    "f.close()\n",
    "\n",
    "def remove_noise(input_text):\n",
    "    words = input_text.split() \n",
    "    noise_free_words = [word for word in words if word not in noise_list] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "\n",
    "# remove_noise(\"this function removes noise from the list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90233 positive tidy tweets saved for training \n",
      "\n",
      "91088 negative tidy tweets saved for training \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "def nlp(tweet):\n",
    "    tweet = remove_pattern(tweet)\n",
    "    tweet = standardization(tweet)\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    tweet = lemmatization(tweet)\n",
    "    tweet = remove_short(tweet)\n",
    "    tweet = remove_noise(tweet)\n",
    "    return tweet\n",
    "\n",
    "f= open(\"twitter-datasets/train_pos_tidy_8.txt\",\"w+\")\n",
    "prev_tweet = ''\n",
    "num = 0\n",
    "for i, tweet in enumerate(train_pos):\n",
    "    if tweet != prev_tweet:\n",
    "        tidy_tweet = nlp(tweet)\n",
    "        f.write(tidy_tweet)\n",
    "        f.write(\"\\n\")\n",
    "        num += 1\n",
    "    prev_tweet = tweet\n",
    "f.close()\n",
    "print(num,\"positive tidy tweets saved for training\",\"\\n\")\n",
    "\n",
    "f= open(\"twitter-datasets/train_neg_tidy_8.txt\",\"w+\")\n",
    "prev_tweet = ''\n",
    "num = 0\n",
    "for i, tweet in enumerate(train_neg):\n",
    "    if tweet != prev_tweet:\n",
    "        tidy_tweet = nlp(tweet)\n",
    "        f.write(tidy_tweet)\n",
    "        f.write(\"\\n\")\n",
    "        num += 1\n",
    "    prev_tweet = tweet\n",
    "f.close()\n",
    "print(num,\"negative tidy tweets saved for training\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
